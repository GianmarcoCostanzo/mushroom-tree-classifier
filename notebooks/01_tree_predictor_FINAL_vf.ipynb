{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 ‚Äì Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 ‚Äì Load Dataset\n",
    "df = pd.read_csv(\"secondary_data.csv\", sep=\";\")\n",
    "print(df.shape)\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 ‚Äì Node Definition\n",
    "class Node:\n",
    "    def __init__(self, is_leaf=False, prediction=None, test_function=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.test_function = test_function\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_leaf:\n",
    "            return self.prediction\n",
    "        elif self.test_function is not None:\n",
    "            if self.test_function(x):\n",
    "                return self.left.predict(x)\n",
    "            else:\n",
    "                return self.right.predict(x)\n",
    "        else:\n",
    "            raise Exception(\"Invalid node: no test function and not a leaf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreePredictor:\n",
    "    def __init__(self, impurity, max_depth=11, min_samples_split=2, max_splits=100):\n",
    "        self.impurity = impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_splits = max_splits\n",
    "        self.num_splits = 0\n",
    "        self.root = None\n",
    "\n",
    "        if impurity == \"gini\":\n",
    "            self.impurity_fn = gini_impurity\n",
    "        elif impurity == \"entropy\":\n",
    "            self.impurity_fn = entropy\n",
    "        elif impurity in [\"misclassification\", \"error\"]:\n",
    "            self.impurity_fn = misclassification_error\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown impurity: {impurity}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.num_splits = 0\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth, available_features=None):\n",
    "        if available_features is None:\n",
    "            available_features = X.columns.tolist()\n",
    "\n",
    "        print(f\"üìê Depth {depth} | Samples: {len(y)}\")\n",
    "\n",
    "        if len(y) < self.min_samples_split or depth >= self.max_depth or y.nunique() == 1:\n",
    "            prediction = y.mode().iloc[0]\n",
    "            return Node(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        best_feature, best_criterion, best_score, left_idx, right_idx = self._choose_split(X, y, available_features)\n",
    "\n",
    "        if best_feature is None or len(left_idx) == 0 or len(right_idx) == 0 or y.empty:\n",
    "            prediction = y.mode().iloc[0] if not y.empty else None\n",
    "            return Node(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        self.num_splits += 1\n",
    "\n",
    "        left_node = self._build_tree(X.iloc[left_idx], y.iloc[left_idx], depth + 1, available_features)\n",
    "        right_node = self._build_tree(X.iloc[right_idx], y.iloc[right_idx], depth + 1, available_features)\n",
    "\n",
    "        return Node(\n",
    "            is_leaf=False,\n",
    "            test_function=best_criterion,\n",
    "            feature_index=best_feature,\n",
    "            left=left_node,\n",
    "            right=right_node\n",
    "        )\n",
    "\n",
    "    def _choose_split(self, X, y, features):\n",
    "        numerical_features = [f for f in features if np.issubdtype(X[f].dtype, np.number)]\n",
    "        categorical_features = [f for f in features if not np.issubdtype(X[f].dtype, np.number)]\n",
    "\n",
    "        df_numerical = get_all_best_splits_numerical(X, y, numerical_features, self.impurity_fn)\n",
    "        df_categorical = get_all_best_splits_categorical(X, y, categorical_features, self.impurity_fn)\n",
    "\n",
    "        df_all = pd.concat([df_numerical, df_categorical], ignore_index=True)\n",
    "        if df_all.empty:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        best_row = df_all.loc[df_all['impurity'].idxmin()]\n",
    "        best_feature = best_row['feature']\n",
    "        best_threshold = best_row.get('best_threshold', np.nan)\n",
    "        best_categories = best_row.get('best_categories', np.nan)\n",
    "\n",
    "        if not pd.isna(best_threshold):\n",
    "            test_function = lambda x, t=best_threshold: x[best_feature] < t\n",
    "            left_mask = X[best_feature] < best_threshold\n",
    "        else:\n",
    "            test_function = lambda x, cat=best_categories: x[best_feature] in cat\n",
    "            left_mask = X[best_feature].isin(best_categories)\n",
    "\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        return (\n",
    "            best_feature,\n",
    "            test_function,\n",
    "            best_row['impurity'],\n",
    "            left_mask[left_mask].index.tolist(),\n",
    "            right_mask[right_mask].index.tolist()\n",
    "        )\n",
    "        \n",
    "    def predict_single(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if node.test_function(x):\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = X.apply(self.predict_single, axis=1)\n",
    "        return (predictions != y).mean()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x) for _, x in X.iterrows()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d41635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 ‚Äì Impurity Functions (Corrected)\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    counts = labels.value_counts(normalize=True)\n",
    "    return 1 - sum(counts ** 2)\n",
    "\n",
    "def entropy(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    counts = labels.value_counts(normalize=True)\n",
    "    return -sum(p * np.log2(p) for p in counts if p > 0)\n",
    "   \n",
    "def misclassification_error(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    counts = labels.value_counts(normalize=True)\n",
    "    return 1 - counts.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe906d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 ‚Äì Best Split for Numerical Feature\n",
    "def get_best_split_numerical(X, y, feature, impurity_fn):\n",
    "    print(f\"üîç Evaluating feature: {feature}\")\n",
    "    data = pd.DataFrame({feature: X[feature], 'label': y}).dropna()\n",
    "    data = data.sort_values(by=feature)\n",
    "    values = data[feature].values\n",
    "    labels = data['label'].values\n",
    "\n",
    "    thresholds = np.unique(values)\n",
    "    thresholds = thresholds[::4]\n",
    "\n",
    "    best_impurity = float('inf')\n",
    "    best_threshold = None\n",
    "    best_test_function = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        left_mask = values <= threshold\n",
    "        right_mask = values > threshold\n",
    "\n",
    "        y_left = labels[left_mask]\n",
    "        y_right = labels[right_mask]\n",
    "\n",
    "        impurity_left = impurity_fn(pd.Series(y_left))\n",
    "        impurity_right = impurity_fn(pd.Series(y_right))\n",
    "\n",
    "        weighted_impurity = (\n",
    "            len(y_left) / len(labels) * impurity_left +\n",
    "            len(y_right) / len(labels) * impurity_right\n",
    "        )\n",
    "\n",
    "        if weighted_impurity < best_impurity:\n",
    "            best_impurity = weighted_impurity\n",
    "            best_threshold = threshold\n",
    "            best_test_function = lambda x, t=threshold: x[feature] > t\n",
    "\n",
    "    return best_threshold, best_impurity, best_test_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 ‚Äì All Best Splits for Numerical Features\n",
    "def get_all_best_splits_numerical(X, y, numerical_features, impurity_fn):\n",
    "    results = []\n",
    "    if not numerical_features:\n",
    "        return pd.DataFrame(columns=['feature', 'best_threshold', 'impurity'])\n",
    "    for feature in numerical_features:\n",
    "        threshold, impurity, test_fn = get_best_split_numerical(X, y, feature, impurity_fn)\n",
    "        results.append({\n",
    "            \"feature\": feature,\n",
    "            \"best_threshold\": threshold,\n",
    "            \"impurity\": impurity\n",
    "        })\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=['feature', 'best_threshold', 'impurity'])\n",
    "    return pd.DataFrame(results).sort_values(by=\"impurity\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 ‚Äì Best Split for Categorical Feature\n",
    "def get_best_split_categorical(X, y, feature, impurity_fn):\n",
    "    data = pd.DataFrame({feature: X[feature], 'label': y}).dropna()\n",
    "    unique_values = data[feature].unique()\n",
    "\n",
    "    best_impurity = float('inf')\n",
    "    best_value = None\n",
    "    best_test_function = None\n",
    "\n",
    "    for val in unique_values:\n",
    "        left_mask = data[feature] == val\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        y_left = data['label'][left_mask]\n",
    "        y_right = data['label'][right_mask]\n",
    "\n",
    "        impurity_left = impurity_fn(y_left)\n",
    "        impurity_right = impurity_fn(y_right)\n",
    "\n",
    "        weighted_impurity = (\n",
    "            len(y_left) / len(data) * impurity_left +\n",
    "            len(y_right) / len(data) * impurity_right\n",
    "        )\n",
    "\n",
    "        if weighted_impurity < best_impurity:\n",
    "            best_impurity = weighted_impurity\n",
    "            best_value = val\n",
    "            best_test_function = lambda x, v=val: x[feature] == v\n",
    "\n",
    "    return best_value, best_impurity, best_test_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493dc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 ‚Äì All Best Splits for Categorical Features\n",
    "def get_all_best_splits_categorical(X, y, categorical_features, impurity_fn):\n",
    "    results = []\n",
    "    if not categorical_features:\n",
    "        return pd.DataFrame(columns=['feature', 'best_value', 'impurity'])\n",
    "    for feature in categorical_features:\n",
    "        value, impurity, test_fn = get_best_split_categorical(X, y, feature, impurity_fn)\n",
    "        results.append({\n",
    "            \"feature\": feature,\n",
    "            \"best_value\": value,\n",
    "            \"impurity\": impurity\n",
    "        })\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=['feature', 'best_value', 'impurity'])\n",
    "    return pd.DataFrame(results).sort_values(by=\"impurity\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4442a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 ‚Äì _build_tree method\n",
    "def _build_tree(self, X, y, depth, available_features=None):\n",
    "    if available_features is None:\n",
    "        available_features = X.columns.tolist()\n",
    "    print(f\"üìê Depth {depth} | Samples: {len(y)}\")\n",
    "    if len(y) < self.min_samples_split or depth >= self.max_depth or y.nunique() == 1:\n",
    "        prediction = y.mode().iloc[0] if not y.empty else None\n",
    "        return Node(is_leaf=True, prediction=prediction)\n",
    "\n",
    "    numerical_features = [f for f in X.select_dtypes(include=[\"float64\", \"int64\"]).columns if f in available_features]\n",
    "    categorical_features = [f for f in X.select_dtypes(include=[\"object\", \"category\"]).columns if f in available_features]\n",
    "\n",
    "    numeric_results = get_all_best_splits_numerical(X, y, numerical_features, self.impurity_fn)\n",
    "    numeric_results[\"split_type\"] = \"numerical\"\n",
    "\n",
    "    categorical_results = get_all_best_splits_categorical(X, y, categorical_features, self.impurity_fn)\n",
    "    categorical_results[\"split_type\"] = \"categorical\"\n",
    "\n",
    "    dfs = [df for df in [numeric_results, categorical_results] if not df.empty]\n",
    "    all_results = pd.concat(dfs, ignore_index=True)\n",
    "    best = all_results.iloc[all_results[\"impurity\"].idxmin()]\n",
    "    best_feature = best['feature']\n",
    "    split_type = best['split_type']\n",
    "    print(f\"   ‚û§ Best split on '{best_feature}' ({split_type}) with impurity = {best['impurity']:.4f}\")\n",
    "    best_feature = best[\"feature\"]\n",
    "    split_type = best[\"split_type\"]\n",
    "\n",
    "    if split_type == \"numerical\":\n",
    "        threshold = best[\"best_threshold\"]\n",
    "        test_fn = lambda x, t=threshold: x[best_feature] > t\n",
    "        left_mask = X[best_feature] > threshold\n",
    "    else:\n",
    "        value = best[\"best_value\"]\n",
    "        test_fn = lambda x, v=value: x[best_feature] == v\n",
    "        left_mask = X[best_feature] == value\n",
    "\n",
    "    right_mask = ~left_mask\n",
    "    X_left, y_left = X[left_mask], y[left_mask]\n",
    "    X_right, y_right = X[right_mask], y[right_mask]\n",
    "\n",
    "    node = Node(is_leaf=False, test_function=test_fn)\n",
    "    node.left = self._build_tree(X_left, y_left, depth + 1, [f for f in available_features if f != best_feature])\n",
    "    node.right = self._build_tree(X_right, y_right, depth + 1, [f for f in available_features if f != best_feature])\n",
    "    return node\n",
    "\n",
    "TreePredictor._build_tree = _build_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 ‚Äì Training Loss (0-1) for Each Impurity Function\n",
    "\n",
    "def compute_training_error(X, y, impurity_list, max_depth=12, min_samples_split=100):\n",
    "    results = {}\n",
    "    for impurity in impurity_list:\n",
    "        print(f\"üîç Training with impurity: {impurity}\")\n",
    "        model = TreePredictor(impurity=impurity, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        model.fit(X, y)\n",
    "        loss = model.evaluate(X, y)\n",
    "        print(f\"  ‚û§ 0-1 Loss: {loss:.4f}\")\n",
    "        results[impurity] = loss\n",
    "    return results\n",
    "\n",
    "def plot_training_errors(loss_dict):\n",
    "    impurities = list(loss_dict.keys())\n",
    "    losses = list(loss_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(impurities, losses, color=[\"#4CAF50\", \"#2196F3\", \"#FF9800\"])\n",
    "    plt.title(\"0-1 Training Loss for Each Impurity Function\")\n",
    "    plt.ylabel(\"0-1 Loss\")\n",
    "    plt.xlabel(\"Impurity Function\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 ‚Äì Select Best Impurity and Nested Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def select_best_impurity(loss_dict):\n",
    "    best_impurity = min(loss_dict, key=loss_dict.get)\n",
    "    print(f\"‚úÖ Best impurity based on training loss: {best_impurity} (loss = {loss_dict[best_impurity]:.4f})\")\n",
    "    return best_impurity\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def nested_cross_validation(X, y, impurity, inner_folds_list, max_depth_grid, min_split_grid, outer_folds=5):\n",
    "    print(f\"üîÅ Starting nested cross-validation for impurity: {impurity}\")\n",
    "    outer_kf = KFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "    outer_scores = []\n",
    "    chosen_params = []\n",
    "\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(outer_kf.split(X), 1):\n",
    "        print(f\"üî∏ Outer Fold {fold_id}/{outer_folds}\")\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "\n",
    "        for inner_k in inner_folds_list:\n",
    "            inner_kf = KFold(n_splits=inner_k, shuffle=True, random_state=fold_id)\n",
    "            for max_depth in max_depth_grid:\n",
    "                for min_split in min_split_grid:\n",
    "                    inner_scores = []\n",
    "\n",
    "                    for inner_train_idx, val_idx in inner_kf.split(X_train):\n",
    "                        X_inner_train, X_val = X_train.iloc[inner_train_idx], X_train.iloc[val_idx]\n",
    "                        y_inner_train, y_val = y_train.iloc[inner_train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                        model = TreePredictor(impurity=impurity, max_depth=max_depth, min_samples_split=min_split)\n",
    "                        model.fit(X_inner_train, y_inner_train)\n",
    "                        acc = 1 - model.evaluate(X_val, y_val)\n",
    "                        inner_scores.append(acc)\n",
    "\n",
    "                    mean_score = np.mean(inner_scores)\n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_params = (max_depth, min_split)\n",
    "\n",
    "        best_max_depth, best_min_split = best_params\n",
    "        print(f\"  ‚û§ Best params: depth={best_max_depth}, split={best_min_split}\")\n",
    "        chosen_params.append(best_params)\n",
    "\n",
    "        final_model = TreePredictor(impurity=impurity, max_depth=best_max_depth, min_samples_split=best_min_split)\n",
    "        final_model.fit(X_train, y_train)\n",
    "        outer_acc = 1 - final_model.evaluate(X_test, y_test)\n",
    "        outer_scores.append(outer_acc)\n",
    "        print(f\"  ‚û§ Outer Fold Accuracy: {outer_acc:.4f}\")\n",
    "\n",
    "    most_common = Counter(chosen_params).most_common(1)[0]\n",
    "    print(f\"üèÜ Parametri migliori globali: depth={most_common[0][0]}, split={most_common[0][1]} (scelti {most_common[1]} volte)\")\n",
    "\n",
    "    return {\n",
    "        \"mean_accuracy\": np.mean(outer_scores),\n",
    "        \"std_accuracy\": np.std(outer_scores),\n",
    "        \"outer_scores\": outer_scores,\n",
    "        \"best_depth\": most_common[0][0],\n",
    "        \"best_min_split\": most_common[0][1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13 ‚Äì Save Nested CV Results\n",
    "def save_nested_cv_outputs(result_dict, impurity, filename_prefix=\"final_nested_cv\"):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"impurity\": [impurity],\n",
    "        \"mean_accuracy\": [result_dict[\"mean_accuracy\"]],\n",
    "        \"std_accuracy\": [result_dict[\"std_accuracy\"]]\n",
    "    })\n",
    "\n",
    "    csv_path = f\"{filename_prefix}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"üíæ Results saved to {csv_path}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([impurity], [result_dict[\"mean_accuracy\"]], yerr=[result_dict[\"std_accuracy\"]],\n",
    "            capsize=8, color=\"#4CAF50\")\n",
    "    plt.title(\"Final Nested CV Accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{filename_prefix}.png\"\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    print(f\"üñºÔ∏è Plot saved to {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5593e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ FINAL EXECUTION ‚Äì Full pipeline with Gini (or best) impurity\n",
    "\n",
    "# Step A ‚Äì Compute training errors for each impurity\n",
    "impurities = [\"gini\", \"entropy\", \"error\"]\n",
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "training_losses = compute_training_error(X, y, impurities, max_depth=12, min_samples_split=500)\n",
    "\n",
    "# Step B ‚Äì Plot 0-1 loss and select best impurity\n",
    "plot_training_errors(training_losses)\n",
    "best_impurity = select_best_impurity(training_losses)\n",
    "\n",
    "# Step C ‚Äì Run nested CV on best impurity\n",
    "nested_results = nested_cross_validation(\n",
    "    X, y,\n",
    "    impurity=best_impurity,\n",
    "    inner_folds_list=[2],\n",
    "    min_split_grid=[100, 500],\n",
    "    max_depth_grid = [11,12],\n",
    "    outer_folds=5\n",
    ")\n",
    "\n",
    "# Step D ‚Äì Save final results\n",
    "save_nested_cv_outputs(nested_results, impurity=best_impurity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9cc7e-e56a-4241-b5bd-3cf08ceb12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = TreePredictor(\n",
    "    impurity=\"gini\",  # oppure usa la variabile best_impurity\n",
    "    max_depth=12,\n",
    "    min_samples_split=100\n",
    ")\n",
    "final_model.fit(X, y)\n",
    "\n",
    "final_error = final_model.evaluate(X, y)\n",
    "print(f\"üìâ Final training error (0-1 loss): {final_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892e2f0-59c6-4a60-921c-c685cb74a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Parametri ottimali trovati nella nested CV\n",
    "best_depth = 12\n",
    "best_min_split = 100\n",
    "best_impurity = \"gini\"  # oppure usa la variabile che avevi prima\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "print(\"üîÅ Cross-validating final model with best hyperparameters...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = TreePredictor(\n",
    "        impurity=best_impurity,\n",
    "        max_depth=best_depth,\n",
    "        min_samples_split=best_min_split\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = 1 - model.evaluate(X_val, y_val)\n",
    "    cv_scores.append(acc)\n",
    "    print(f\"  ‚úÖ Fold {fold} accuracy: {acc:.4f}\")\n",
    "\n",
    "# Statistiche finali\n",
    "mean_cv_acc = np.mean(cv_scores)\n",
    "std_cv_acc = np.std(cv_scores)\n",
    "\n",
    "print(f\"\\nüìä Final CV Accuracy: {mean_cv_acc:.4f} ¬± {std_cv_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ea1c2-1d95-42ef-bc43-955f0b648bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class RandomForestPredictor:\n",
    "    def __init__(self, n_estimators=10, impurity=\"gini\", max_depth=5, min_samples_split=10, bootstrap=True, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.impurity = impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.bootstrap = bootstrap\n",
    "        self.trees = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                X_sample, y_sample = resample(X, y, random_state=self.random_state + i)\n",
    "            else:\n",
    "                X_sample, y_sample = X, y\n",
    "\n",
    "            tree = TreePredictor(\n",
    "                impurity=self.impurity,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            print(f\"üå≤ Trained tree {i + 1}/{self.n_estimators}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ottieni le predizioni da ogni albero\n",
    "        all_preds = []\n",
    "        for tree in self.trees:\n",
    "            preds = tree.predict(X)\n",
    "            all_preds.append(preds)\n",
    "\n",
    "        # Trasponi la lista: ogni riga = predizioni dei vari alberi per 1 punto\n",
    "        all_preds = np.array(all_preds).T\n",
    "\n",
    "        # Maggioranza\n",
    "        majority_votes = []\n",
    "        for row in all_preds:\n",
    "            vote = Counter(row).most_common(1)[0][0]\n",
    "            majority_votes.append(vote)\n",
    "\n",
    "        return np.array(majority_votes)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred != y_true)  # 0-1 loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7de358-28b3-4c56-80d0-7727be20d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training con 20 alberi\n",
    "rf = RandomForestPredictor(\n",
    "    n_estimators=5,             # üëà puoi cambiare in 10 se vuoi\n",
    "    impurity=\"gini\",\n",
    "    max_depth=12,\n",
    "    min_samples_split=500,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training Random Forest...\")\n",
    "rf.fit(X, y)\n",
    "\n",
    "train_error_rf = rf.evaluate(X, y)\n",
    "print(f\"üå≤ Random Forest training error (0-1 loss): {train_error_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360db32-02cb-4c17-91e9-eed4beb8d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Parametri per la Random Forest\n",
    "n_estimators = 5\n",
    "max_depth = 12\n",
    "min_split = 500\n",
    "impurity = \"gini\"\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_scores = []\n",
    "\n",
    "print(\"üîÅ 5-Fold Cross-Validation on Random Forest...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"\\nüî∏ Fold {fold}/5 - Training Random Forest...\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    rf = RandomForestPredictor(\n",
    "        n_estimators=n_estimators,\n",
    "        impurity=impurity,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_split,\n",
    "        bootstrap=True,\n",
    "        random_state=fold  # per diversificare i bootstrap ad ogni fold\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    acc = 1 - rf.evaluate(X_val, y_val)\n",
    "    rf_scores.append(acc)\n",
    "\n",
    "    print(f\"  ‚úÖ Fold {fold} accuracy: {acc:.4f}\")\n",
    "\n",
    "# Riassunto finale\n",
    "mean_acc = np.mean(rf_scores)\n",
    "std_acc = np.std(rf_scores)\n",
    "\n",
    "print(f\"\\nüìä Random Forest CV Accuracy: {mean_acc:.4f} ¬± {std_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc61e57-3535-4051-9a65-50aaaecb1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
